# """
# ìë™ ë°ì´í„° ìˆ˜ì§‘ê¸°
# RSS í”¼ë“œ + ê³µì‹ APIë¥¼ í†µí•œ ìë™ ìˆ˜ì§‘
# """

# import json
# import requests
# from datetime import datetime, timedelta
# import feedparser
# import re
# from typing import List, Dict
# import os  # ğŸ‘ˆ [ìˆ˜ì • 1] os ëª¨ë“ˆ ì¶”ê°€

# # --- ê²½ë¡œ ì„¤ì • ---
# # ì´ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼(auto_scraper.py)ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
# BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# # ë°ì´í„° íŒŒì¼ ê²½ë¡œ (í•œ ë‹¨ê³„ ìƒìœ„ í´ë”, ì¦‰ í”„ë¡œì íŠ¸ ë£¨íŠ¸)
# DATA_FILE = os.path.join(BASE_DIR, '..', 'data.json')
# # ---------------

# class CareerScraper:
#     def __init__(self):
#         self.data = self.load_existing_data()
#         self.new_items_count = 0
        
#         # í™•ì¥ëœ í‚¤ì›Œë“œ (200ê°œ+)
#         self.keywords = {
#             "ë¬´ì—­": [
#                 # í•µì‹¬
#                 "ë¬´ì—­", "ìˆ˜ì¶œ", "ìˆ˜ì…", "í†µê´€", "ê´€ì„¸", "FTA", "HSì½”ë“œ",
#                 # ê¸°ê´€
#                 "KOTRA", "ë¬´ì—­í˜‘íšŒ", "ê´€ì„¸ì²­", "ë¬´ì—­ë³´í—˜ê³µì‚¬", "ìˆ˜ì¶œì…ì€í–‰",
#                 # ì§ë¬´
#                 "í•´ì™¸ì˜ì—…", "ê¸€ë¡œë²Œì˜ì—…", "êµ­ì œì˜ì—…", "ë°”ì´ì–´", "ì†Œì‹±",
#                 "SCM", "ë¬¼ë¥˜", "í¬ì›Œë”©", "í•´ìš´", "í•­ê³µ", "ì˜ì—…", "ê²€ì—­",
#                 # ì§€ì—­
#                 "ì¤‘êµ­ë¬´ì—­", "ë¯¸êµ­ìˆ˜ì¶œ", "ìœ ëŸ½", "ë™ë‚¨ì•„", "RCEP",
#                 # ê¸°íƒ€
#                 "e-ì»¤ë¨¸ìŠ¤", "í¬ë¡œìŠ¤ë³´ë”", "êµ­ì œë°°ì†¡", "ì•„ë§ˆì¡´"
#             ],
#             "ê²½ì œ": [
#                 # í•µì‹¬
#                 "ê²½ì œ", "ê¸ˆìœµ", "ì€í–‰", "ì¦ê¶Œ", "ìì‚°ìš´ìš©", "íˆ¬ì", "í€ë“œ",
#                 "IB", "ì• ë„ë¦¬ìŠ¤íŠ¸", "ë¦¬ì„œì¹˜", "ì¬ë¬´", "íšŒê³„", "ì„¸ë¬´",
#                 # ê¸ˆìœµê¸°ê´€
#                 "KB", "ì‹ í•œ", "í•˜ë‚˜", "ìš°ë¦¬", "NHë†í˜‘", "IBKê¸°ì—…", "KDBì‚°ì—…",
#                 "ë¯¸ë˜ì—ì…‹", "ì‚¼ì„±ì¦ê¶Œ", "í•œêµ­íˆ¬ì", "í•œêµ­ê±°ë˜ì†Œ", "í•œêµ­ì€í–‰",
#                 # ì§ë¬´
#                 "PB", "WM", "ìì‚°ê´€ë¦¬", "ì‹ ìš©í‰ê°€", "ì‹¬ì‚¬ì—­", "ì—¬ì‹ ",
#                 "ê¸ˆìœµì¼ë°˜", "ì±„ê¶Œ", "ì£¼ì‹", "ì™¸í™˜", "ë¦¬ì„œì¹˜", "ë¦¬ìŠ¤í¬ê´€ë¦¬",
#                 "ê²½ì˜ê¸°íš", "ì „ëµê¸°íš", "IR", "M&A",
#                 # ìê²©ì¦
#                 "CPA", "AICPA", "CFA", "FRM", "CFP",
#                 # ê¸°íƒ€
#                 "ESG", "í•€í…Œí¬", "ë””ì§€í„¸ê¸ˆìœµ", "ë¸”ë¡ì²´ì¸"
#             ],
#             "ì •ì¹˜ì™¸êµ": [
#                 # í•µì‹¬
#                 "ì™¸êµ", "êµ­ì œê´€ê³„", "ì •ì¹˜", "êµ­ì œì •ì¹˜", "ì•ˆë³´", "í†µì¼",
#                 "ì™¸êµê´€", "í–‰ì •ê³ ì‹œ", "ì™¸ë¬´ê³ ì‹œ", "5ê¸‰ê³µì±„",
#                 # ê¸°ê´€
#                 "ì™¸êµë¶€", "í†µì¼ë¶€", "êµ­ë°©ë¶€", "ì²­ì™€ëŒ€", "êµ­íšŒ",
#                 "UN", "ìœ ì—”", "UNDP", "UNESCO", "WHO", "UNICEF",
#                 "NATO", "EU", "ASEAN", "APEC", "G20", "OECD",
#                 "ì„¸ê³„ì€í–‰", "IMF", "WTO",
#                 # NGO
#                 "NGO", "INGO", "ì˜¥ìŠ¤íŒœ", "ì›”ë“œë¹„ì „", "êµ¿ë„¤ì´ë²„ìŠ¤",
#                 "êµ­ê²½ì—†ëŠ”ì˜ì‚¬íšŒ", "KOICA", "ì›”ë“œí”„ë Œì¦ˆ",
#                 # ì§ë¬´
#                 "ê³µê³µì™¸êµ", "ë‹¤ìì™¸êµ", "ê²½ì œì™¸êµ", "ë¬¸í™”ì™¸êµ",
#                 "ê°œë°œí˜‘ë ¥", "ODA", "í‰í™”", "ê°ˆë“±í•´ê²°", "ì¸ê¶Œ",
#                 "ì •ì±…ì—°êµ¬", "ì‹±í¬íƒ±í¬",
#                 # ì§€ì—­
#                 "ë¶í•µ", "í•œë°˜ë„", "ë™ë¶ì•„", "ì¤‘ë™", "í•œë¯¸ë™ë§¹",
#                 # ê¸°íƒ€
#                 "ëŒ€ì‚¬ê´€", "ì˜ì‚¬ê´€", "êµ­ì œíšŒì˜", "SDGs", "ê¸°í›„ë³€í™”"
#             ]
#         }
    
#     def load_existing_data(self) -> Dict:
#         """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
#         try:
#             # ğŸ‘ˆ [ìˆ˜ì • 2] data.json ê²½ë¡œ ìˆ˜ì •
#             with open(DATA_FILE, 'r', encoding='utf-8') as f:
#                 return json.load(f)
#         except FileNotFoundError:
#             return {"lastUpdated": "", "jobs": [], "contests": []}
    
#     def categorize(self, text: str) -> str:
#         """í…ìŠ¤íŠ¸ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
#         text_lower = text.lower()
#         scores = {category: 0 for category in self.keywords}
        
#         for category, keywords in self.keywords.items():
#             for keyword in keywords:
#                 if keyword in text_lower:
#                     scores[category] += 1
        
#         max_score = max(scores.values())
#         if max_score > 0:
#             return max(scores, key=scores.get)
#         return None  # ê´€ë ¨ ì—†ìŒ
    
#     def is_duplicate(self, new_item: Dict, existing_items: List[Dict]) -> bool:
#         """ì¤‘ë³µ í™•ì¸"""
#         for item in existing_items:
#             if item.get('url') == new_item.get('url'):
#                 return True
#             if item.get('title') == new_item.get('title'):
#                 return True
#         return False
    
#     def parse_rss_feeds(self):
#         """RSS í”¼ë“œ ìˆ˜ì§‘"""
#         print("ğŸ“¡ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
        
#         # ì‹¤ì œ RSS URLë¡œ êµì²´í•˜ì„¸ìš”
#         rss_sources = {
#             "jobs": [
#                 "https://www.saramin.co.kr/zf_user/help/live/rss",         # ì‚¬ëŒì¸ 
#                 "https://rss.incruit.com/list/TodayNew.asp",        # ì¸í¬ë£¨íŠ¸ 
#                 "http://rss.campusmon.com/rss/newrecruit.asp"       # ìº í¼ìŠ¤ëª¬ 
#             ],
#             "contests": [
#                 "https://www.wevity.com/index_rss.php",              # ìœ„ë¹„í‹° 
#                 "https://www.thinkcontest.com/rss/rss_thinkcontest.xml", # ì”½ìœ  
#                 "https://www.campus-pick.com/api/v1/contest/rss",   # ìº í¼ìŠ¤í”½ 
#                 "https://www.all-con.co.kr/rss/allcontest.xml"      # ì˜¬ì½˜ 
#             ]
#         }
        
#         for feed_url in rss_sources["jobs"]:
#             try:
#                 feed = feedparser.parse(feed_url)
#                 for entry in feed.entries[:20]:
#                     title = entry.get('title', '')
#                     description = entry.get('summary', '')
                    
#                     category = self.categorize(title + " " + description)
#                     if not category:  # ê´€ë ¨ ì—†ìœ¼ë©´ ìŠ¤í‚µ
#                         continue
                    
#                     job = {
#                         "id": len(self.data["jobs"]) + self.new_items_count + 1,
#                         "title": title,
#                         "company": entry.get('author', 'ë¯¸ì •'),
#                         "category": category,
#                         "type": "ì±„ìš©",
#                         "deadline": self.extract_deadline(entry),
#                         "url": entry.get('link', ''),
#                         "tags": self.extract_tags(title),
#                         "new": True
#                     }
                    
#                     if not self.is_duplicate(job, self.data["jobs"]):
#                         self.data["jobs"].append(job)
#                         self.new_items_count += 1
#                         print(f"âœ… ìƒˆ ì±„ìš©: {title[:40]}...")
#             except Exception as e:
#                 print(f"âŒ RSS ì˜¤ë¥˜: {e}")
        
#         for feed_url in rss_sources["contests"]:
#             try:
#                 feed = feedparser.parse(feed_url)
#                 for entry in feed.entries[:20]:
#                     title = entry.get('title', '')
#                     description = entry.get('summary', '')
                    
#                     category = self.categorize(title + " " + description)
#                     if not category:
#                         continue
                    
#                     contest = {
#                         "id": len(self.data["contests"]) + self.new_items_count + 1,
#                         "title": title,
#                         "organizer": entry.get('author', 'ë¯¸ì •'),
#                         "category": category,
#                         "type": "ê³µëª¨ì „",
#                         "deadline": self.extract_deadline(entry),
#                         "prize": self.extract_prize(description),
#                         "url": entry.get('link', ''),
#                         "tags": self.extract_tags(title),
#                         "new": True
#                     }
                    
#                     if not self.is_duplicate(contest, self.data["contests"]):
#                         self.data["contests"].append(contest)
#                         self.new_items_count += 1
#                         print(f"âœ… ìƒˆ ê³µëª¨ì „: {title[:40]}...")
#             except Exception as e:
#                 print(f"âŒ RSS ì˜¤ë¥˜: {e}")
    
#     def extract_deadline(self, entry) -> str:
#         """ë§ˆê°ì¼ ì¶”ì¶œ"""
#         future_date = datetime.now() + timedelta(days=30)
#         return future_date.strftime('%Y-%m-%d')
    
#     def extract_prize(self, text: str) -> str:
#         """ìƒê¸ˆ ì¶”ì¶œ"""
#         patterns = [r'(ëŒ€ìƒ|ìƒê¸ˆ)[:\s]*([0-9,]+ë§Œ?\s?ì›)', r'([0-9,]+ë§Œ?\s?ì›)']
#         for pattern in patterns:
#             match = re.search(pattern, text)
#             if match:
#                 return match.group(0)
#         return ""
    
#     def extract_tags(self, text: str) -> List[str]:
#         """íƒœê·¸ ì¶”ì¶œ"""
#         tags = []
#         tag_keywords = {
#             "ì‹ ì…", "ê²½ë ¥", "ì¸í„´", "ëŒ€í•™ìƒ", "ì²­ë…„",
#             "ì„œìš¸", "ë¶€ì‚°", "ì˜¨ë¼ì¸", "ì•„ì´ë””ì–´"
#         }
#         text_lower = text.lower()
#         for keyword in tag_keywords:
#             if keyword in text_lower:
#                 tags.append(keyword)
#         return tags[:5]
    
#     def clean_old_data(self):
#         """ì§€ë‚œ ê³µê³  ì œê±°"""
#         print("ğŸ§¹ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì¤‘...")
#         today = datetime.now()
        
#         self.data["jobs"] = [
#             job for job in self.data["jobs"]
#             if datetime.strptime(job['deadline'], '%Y-%m-%d') >= today
#         ]
        
#         self.data["contests"] = [
#             contest for contest in self.data["contests"]
#             if datetime.strptime(contest['deadline'], '%Y-%m-%d') >= today
#         ]
    
#     def save_data(self):
#         """ë°ì´í„° ì €ì¥"""
#         self.data['lastUpdated'] = datetime.now().isoformat()
        
#         # ğŸ‘ˆ [ìˆ˜ì • 3] data.json ê²½ë¡œ ìˆ˜ì •
#         with open(DATA_FILE, 'w', encoding='utf-8') as f:
#             json.dump(self.data, f, ensure_ascii=False, indent=2)
        
#         print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ! (ê²½ë¡œ: {DATA_FILE})")
#         print(f"ğŸ“Š ì±„ìš©: {len(self.data['jobs'])}ê±´")
#         print(f"ğŸ† ê³µëª¨ì „: {len(self.data['contests'])}ê±´")
#         print(f"ğŸ†• ì‹ ê·œ: {self.new_items_count}ê±´")

# def main():
#     print("=" * 50)
#     print("ğŸ¤– ìë™ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
#     print("=" * 50)
    
#     scraper = CareerScraper()
#     scraper.parse_rss_feeds()
#     scraper.clean_old_data()
#     scraper.save_data()
    
#     print("\nâœ… ìˆ˜ì§‘ ì™„ë£Œ!")

# if __name__ == "__main__":
#     main()


"""
ìë™ ë°ì´í„° ìˆ˜ì§‘ê¸°
RSS í”¼ë“œ + ê³µì‹ APIë¥¼ í†µí•œ ìë™ ìˆ˜ì§‘
(v_PNU_fix - ì ‘ì† ë¬¸ì œ ë° ë²„ê·¸ ìˆ˜ì • ìµœì¢…ë³¸)
"""

import json
import requests
from datetime import datetime, timedelta
import feedparser  # ğŸ‘ˆ feedparserë§Œ import
import re
from typing import List, Dict
import os

# --- ê²½ë¡œ ì„¤ì • ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_FILE = os.path.join(BASE_DIR, '..', 'data.json')
# ---------------

# ğŸ‘ˆ [ì¶”ê°€!] ë´‡ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•œ 'ì‹ ë¶„ì¦' (User-Agent)
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

class CareerScraper:
    def __init__(self):
        self.data = self.load_existing_data()
        self.new_items_count = 0
        self.keywords = {
            "ë¬´ì—­": ["ë¬´ì—­", "ìˆ˜ì¶œ", "ìˆ˜ì…", "í†µê´€", "ê´€ì„¸", "FTA", "HSì½”ë“œ", "KOTRA", "ë¬´ì—­í˜‘íšŒ", "ê´€ì„¸ì²­", "ë¬´ì—­ë³´í—˜ê³µì‚¬", "ìˆ˜ì¶œì…ì€í–‰", "í•´ì™¸ì˜ì—…", "ê¸€ë¡œë²Œì˜ì—…", "êµ­ì œì˜ì—…", "ë°”ì´ì–´", "ì†Œì‹±", "SCM", "ë¬¼ë¥˜", "í¬ì›Œë”©", "í•´ìš´", "í•­ê³µ", "ì˜ì—…", "ê²€ì—­", "ì¤‘êµ­ë¬´ì—­", "ë¯¸êµ­ìˆ˜ì¶œ", "ìœ ëŸ½", "ë™ë‚¨ì•„", "RCEP", "e-ì»¤ë¨¸ìŠ¤", "í¬ë¡œìŠ¤ë³´ë”", "êµ­ì œë°°ì†¡", "ì•„ë§ˆì¡´"],
            "ê²½ì œ": ["ê²½ì œ", "ê¸ˆìœµ", "ì€í–‰", "ì¦ê¶Œ", "ìì‚°ìš´ìš©", "íˆ¬ì", "í€ë“œ", "IB", "ì• ë„ë¦¬ìŠ¤íŠ¸", "ë¦¬ì„œì¹˜", "ì¬ë¬´", "íšŒê³„", "ì„¸ë¬´", "KB", "ì‹ í•œ", "í•˜ë‚˜", "ìš°ë¦¬", "NHë†í˜‘", "IBKê¸°ì—…", "KDBì‚°ì—…", "ë¯¸ë˜ì—ì…‹", "ì‚¼ì„±ì¦ê¶Œ", "í•œêµ­íˆ¬ì", "í•œêµ­ê±°ë˜ì†Œ", "í•œêµ­ì€í–‰", "PB", "WM", "ìì‚°ê´€ë¦¬", "ì‹ ìš©í‰ê°€", "ì‹¬ì‚¬ì—­", "ì—¬ì‹ ", "ê¸ˆìœµì¼ë°˜", "ì±„ê¶Œ", "ì£¼ì‹", "ì™¸í™˜", "ë¦¬ì„œì¹˜", "ë¦¬ìŠ¤í¬ê´€ë¦¬", "ê²½ì˜ê¸°íš", "ì „ëµê¸°íš", "IR", "M&A", "CPA", "AICPA", "CFA", "FRM", "CFP", "ESG", "í•€í…Œí¬", "ë””ì§€í„¸ê¸ˆìœµ", "ë¸”ë¡ì²´ì¸"],
            "ì •ì¹˜ì™¸êµ": ["ì™¸êµ", "êµ­ì œê´€ê³„", "ì •ì¹˜", "êµ­ì œì •ì¹˜", "ì•ˆë³´", "í†µì¼", "ì™¸êµê´€", "í–‰ì •ê³ ì‹œ", "ì™¸ë¬´ê³ ì‹œ", "5ê¸‰ê³µì±„", "ì™¸êµë¶€", "í†µì¼ë¶€", "êµ­ë°©ë¶€", "ì²­ì™€ëŒ€", "êµ­íšŒ", "UN", "ìœ ì—”", "UNDP", "UNESCO", "WHO", "UNICEF", "NATO", "EU", "ASEAN", "APEC", "G20", "OECD", "ì„¸ê³„ì€í–‰", "IMF", "WTO", "NGO", "INGO", "ì˜¥ìŠ¤íŒœ", "ì›”ë“œë¹„ì „", "êµ¿ë„¤ì´ë²„ìŠ¤", "êµ­ê²½ì—†ëŠ”ì˜ì‚¬íšŒ", "KOICA", "ì›”ë“œí”„ë Œì¦ˆ", "ê³µê³µì™¸êµ", "ë‹¤ìì™¸êµ", "ê²½ì œì™¸êµ", "ë¬¸í™”ì™¸êµ", "ê°œë°œí˜‘ë ¥", "ODA", "í‰í™”", "ê°ˆë“±í•´ê²°", "ì¸ê¶Œ", "ì •ì±…ì—°êµ¬", "ì‹±í¬íƒ±í¬", "ë¶í•µ", "í•œë°˜ë„", "ë™ë¶ì•„", "ì¤‘ë™", "í•œë¯¸ë™ë§¹", "ëŒ€ì‚¬ê´€", "ì˜ì‚¬ê´€", "êµ­ì œíšŒì˜", "SDGs", "ê¸°í›„ë³€í™”"]
        }
    
    def load_existing_data(self) -> Dict:
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"â„¹ï¸ 'data.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤. (ê²½ë¡œ: {DATA_FILE})")
            return {"lastUpdated": "", "jobs": [], "contests": []}
    
    def categorize(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        text_lower = text.lower()
        scores = {category: 0 for category in self.keywords}
        
        for category, keywords in self.keywords.items():
            for keyword in keywords:
                # â¬‡ï¸ [ë²„ê·¸ ìˆ˜ì •!] keywordë„ .lower()ë¥¼ ì ìš©
                if keyword.lower() in text_lower:
                    scores[category] += 1
        
        max_score = max(scores.values())
        if max_score > 0:
            return max(scores, key=scores.get)
        return None  # ê´€ë ¨ ì—†ìŒ
    
    def is_duplicate(self, new_item: Dict, existing_items: List[Dict]) -> bool:
        """ì¤‘ë³µ í™•ì¸"""
        for item in existing_items:
            if item.get('url') == new_item.get('url'):
                return True
            if item.get('title') == new_item.get('title'):
                return True
        return False
    
    def parse_rss_feeds(self):
        """RSS í”¼ë“œ ìˆ˜ì§‘"""
        print("ğŸ“¡ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
        
        rss_sources = {
            "jobs": [
                "https://www.pusan.ac.kr/kor/CMS/Board/Board.do?robot=Y&mCode=MN098&type=rss", 
                "https://www.saramin.co.kr/zf_user/help/live/rss",
                "https://rss.incruit.com/list/TodayNew.asp",
                "https://rss.campusmon.com/rss/newrecruit.asp"  # ğŸ‘ˆ [ìˆ˜ì •] http -> https
            ],
            "contests": [
                "https://www.pusan.ac.kr/kor/CMS/Board/Board.do?robot=Y&mCode=MN099&type=rss",
                "https://www.wevity.com/index_rss.php",
                "https://www.thinkcontest.com/rss/rss_thinkcontest.xml",
                "https://www.campus-pick.com/api/v1/contest/rss",
                "https://www.all-con.co.kr/rss/allcontest.xml"
            ]
        }
        
        for feed_url in rss_sources["jobs"]:
            try:
                print(f"  > [ì±„ìš©] {feed_url} í™•ì¸ ì¤‘...")
                is_pnu_feed = "pusan.ac.kr" in feed_url
                
                # ğŸ‘ˆ [ìˆ˜ì •!] 'ì‚¬ëŒì¸ ì²™' ì ‘ì†
                feed = feedparser.parse(feed_url, agent=USER_AGENT)
                
                if feed.bozo: # ğŸ‘ˆ [ì¶”ê°€!] feedparserê°€ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆëŠ”ì§€ í™•ì¸
                    print(f"    âš ï¸ ê²½ê³ : {feed_url} íŒŒì‹± ì‹¤íŒ¨. (bozo=1)")
                    print(f"    {feed.bozo_exception}")
                    continue # ì‹¤íŒ¨í•˜ë©´ ì´ URLì€ ê±´ë„ˆëœ€

                for entry in feed.entries[:20]:
                    title = entry.get('title', '')
                    description = entry.get('summary', '')
                    
                    category = self.categorize(title + " " + description)
                    
                    if not category:
                        if is_pnu_feed:
                            category = "ì •ì¹˜ì™¸êµ"
                        else:
                            continue
                    
                    job = {
                        "id": len(self.data["jobs"]) + self.new_items_count + 1,
                        "title": title,
                        "company": entry.get('author', 'ë¯¸ì •'),
                        "category": category,
                        "type": "ì±„ìš©",
                        "deadline": self.extract_deadline(entry),
                        "url": entry.get('link', ''),
                        "tags": self.extract_tags(title + " " + entry.get('link', '')),
                        "new": True
                    }
                    
                    if not self.is_duplicate(job, self.data["jobs"]):
                        self.data["jobs"].append(job)
                        self.new_items_count += 1
                        print(f"    âœ… ìƒˆ ì±„ìš©: {title[:30]}...")
            except Exception as e:
                print(f"    âŒ RSS ì˜¤ë¥˜ ({feed_url}): {e}")
        
        for feed_url in rss_sources["contests"]:
            try:
                print(f"  > [ê³µëª¨ì „] {feed_url} í™•ì¸ ì¤‘...")
                is_pnu_feed = "pusan.ac.kr" in feed_url
                
                # ğŸ‘ˆ [ìˆ˜ì •!] 'ì‚¬ëŒì¸ ì²™' ì ‘ì†
                feed = feedparser.parse(feed_url, agent=USER_AGENT)

                if feed.bozo: # ğŸ‘ˆ [ì¶”ê°€!] feedparserê°€ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆëŠ”ì§€ í™•ì¸
                    print(f"    âš ï¸ ê²½ê³ : {feed_url} íŒŒì‹± ì‹¤íŒ¨. (bozo=1)")
                    print(f"    {feed.bozo_exception}")
                    continue # ì‹¤íŒ¨í•˜ë©´ ì´ URLì€ ê±´ë„ˆëœ€
                
                for entry in feed.entries[:20]:
                    title = entry.get('title', '')
                    description = entry.get('summary', '')
                    
                    category = self.categorize(title + " " + description)
                    
                    if not category:
                        if is_pnu_feed:
                            category = "ì •ì¹˜ì™¸êµ"
                        else:
                            continue
                    
                    contest = {
                        "id": len(self.data["contests"]) + self.new_items_count + 1,
                        "title": title,
                        "organizer": entry.get('author', 'ë¯¸ì •'),
                        "category": category,
                        "type": "ê³µëª¨ì „",
                        "deadline": self.extract_deadline(entry),
                        "prize": self.extract_prize(description),
                        "url": entry.get('link', ''),
                        "tags": self.extract_tags(title + " " + entry.get('link', '')),
                        "new": True
                    }
                    
                    if not self.is_duplicate(contest, self.data["contests"]):
                        self.data["contests"].append(contest)
                        self.new_items_count += 1
                        print(f"    âœ… ìƒˆ ê³µëª¨ì „: {title[:30]}...")
            except Exception as e:
                print(f"    âŒ RSS ì˜¤ë¥˜ ({feed_url}): {e}")
    
    def extract_deadline(self, entry) -> str:
        """ë§ˆê°ì¼ ì¶”ì¶œ (ì„ì‹œë¡œ 30ì¼)"""
        future_date = datetime.now() + timedelta(days=30)
        return future_date.strftime('%Y-%m-%d')
    
    def extract_prize(self, text: str) -> str:
        """ìƒê¸ˆ ì¶”ì¶œ"""
        patterns = [r'(ëŒ€ìƒ|ìƒê¸ˆ)[:\s]*([0-9,]+ë§Œ?\s?ì›)', r'([0-9,]+ë§Œ?\s?ì›)']
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        return ""
    
    def extract_tags(self, text: str) -> List[str]:
        """íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        tag_keywords = {
            "ì‹ ì…", "ê²½ë ¥", "ì¸í„´", "ëŒ€í•™ìƒ", "ì²­ë…„",
            "ì„œìš¸", "ë¶€ì‚°", "ì˜¨ë¼ì¸", "ì•„ì´ë””ì–´",
            "ë¶€ì‚°ëŒ€"
        }
        text_lower = text.lower()

        if "pusan.ac.kr" in text_lower or "ë¶€ì‚°ëŒ€í•™êµ" in text_lower:
            tags.append("ë¶€ì‚°ëŒ€")
            
        for keyword in tag_keywords:
            if keyword in text_lower:
                if keyword not in tags:
                    tags.append(keyword)
        return tags[:5]
    
    def clean_old_data(self):
        """ì§€ë‚œ ê³µê³  ì œê±°"""
        print("ğŸ§¹ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        today = datetime.now()
        
        def filter_item(item):
            try:
                return datetime.strptime(item['deadline'], '%Y-%m-%d') >= today
            except (ValueError, TypeError):
                return False

        self.data["jobs"] = [job for job in self.data["jobs"] if filter_item(job)]
        self.data["contests"] = [contest for contest in self.data["contests"] if filter_item(contest)]
    
    def save_data(self):
        """ë°ì´í„° ì €ì¥"""
        self.data['lastUpdated'] = datetime.now().isoformat()
        
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ! (ê²½ë¡œ: {DATA_FILE})")
        print(f"ğŸ“Š ì±„ìš©: {len(self.data['jobs'])}ê±´")
        print(f"ğŸ† ê³µëª¨ì „: {len(self.data['contests'])}ê±´")
        print(f"ğŸ†• ì‹ ê·œ: {self.new_items_count}ê±´")

def main():
    print("=" * 50)
    print("ğŸ¤– ìë™ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (v_PNU_fix_v3)")
    print("=" * 50)
    
    scraper = CareerScraper()
    scraper.clean_old_data()
    scraper.parse_rss_feeds()
    scraper.save_data()
    
    print("\nâœ… ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
