"""
ìë™ ë°ì´í„° ìˆ˜ì§‘ê¸°
RSS í”¼ë“œ + ê³µì‹ APIë¥¼ í†µí•œ ìë™ ìˆ˜ì§‘
"""

import json
import requests
from datetime import datetime, timedelta
import feedparser
import re
from typing import List, Dict

class CareerScraper:
    def __init__(self):
        self.data = self.load_existing_data()
        self.new_items_count = 0
        
        # í™•ì¥ëœ í‚¤ì›Œë“œ (200ê°œ+)
        self.keywords = {
            "ë¬´ì—­": [
                # í•µì‹¬
                "ë¬´ì—­", "ìˆ˜ì¶œ", "ìˆ˜ì…", "í†µê´€", "ê´€ì„¸", "FTA", "HSì½”ë“œ",
                # ê¸°ê´€
                "KOTRA", "ë¬´ì—­í˜‘íšŒ", "ê´€ì„¸ì²­", "ë¬´ì—­ë³´í—˜ê³µì‚¬", "ìˆ˜ì¶œì…ì€í–‰",
                # ì§ë¬´
                "í•´ì™¸ì˜ì—…", "ê¸€ë¡œë²Œì˜ì—…", "êµ­ì œì˜ì—…", "ë°”ì´ì–´", "ì†Œì‹±",
                "SCM", "ë¬¼ë¥˜", "í¬ì›Œë”©", "í•´ìš´", "í•­ê³µ", "ì˜ì—…", "ê²€ì—­",
                # ì§€ì—­
                "ì¤‘êµ­ë¬´ì—­", "ë¯¸êµ­ìˆ˜ì¶œ", "ìœ ëŸ½", "ë™ë‚¨ì•„", "RCEP",
                # ê¸°íƒ€
                "e-ì»¤ë¨¸ìŠ¤", "í¬ë¡œìŠ¤ë³´ë”", "êµ­ì œë°°ì†¡", "ì•„ë§ˆì¡´"
            ],
            "ê²½ì œ": [
                # í•µì‹¬
                "ê²½ì œ", "ê¸ˆìœµ", "ì€í–‰", "ì¦ê¶Œ", "ìì‚°ìš´ìš©", "íˆ¬ì", "í€ë“œ",
                "IB", "ì• ë„ë¦¬ìŠ¤íŠ¸", "ë¦¬ì„œì¹˜", "ì¬ë¬´", "íšŒê³„", "ì„¸ë¬´",
                # ê¸ˆìœµê¸°ê´€
                "KB", "ì‹ í•œ", "í•˜ë‚˜", "ìš°ë¦¬", "NHë†í˜‘", "IBKê¸°ì—…", "KDBì‚°ì—…",
                "ë¯¸ë˜ì—ì…‹", "ì‚¼ì„±ì¦ê¶Œ", "í•œêµ­íˆ¬ì", "í•œêµ­ê±°ë˜ì†Œ", "í•œêµ­ì€í–‰",
                # ì§ë¬´
                "PB", "WM", "ìì‚°ê´€ë¦¬", "ì‹ ìš©í‰ê°€", "ì‹¬ì‚¬ì—­", "ì—¬ì‹ ",
                "ê¸ˆìœµì¼ë°˜", "ì±„ê¶Œ", "ì£¼ì‹", "ì™¸í™˜", "ë¦¬ì„œì¹˜", "ë¦¬ìŠ¤í¬ê´€ë¦¬",
                "ê²½ì˜ê¸°íš", "ì „ëµê¸°íš", "IR", "M&A",
                # ìê²©ì¦
                "CPA", "AICPA", "CFA", "FRM", "CFP",
                # ê¸°íƒ€
                "ESG", "í•€í…Œí¬", "ë””ì§€í„¸ê¸ˆìœµ", "ë¸”ë¡ì²´ì¸"
            ],
            "ì •ì¹˜ì™¸êµ": [
                # í•µì‹¬
                "ì™¸êµ", "êµ­ì œê´€ê³„", "ì •ì¹˜", "êµ­ì œì •ì¹˜", "ì•ˆë³´", "í†µì¼",
                "ì™¸êµê´€", "í–‰ì •ê³ ì‹œ", "ì™¸ë¬´ê³ ì‹œ", "5ê¸‰ê³µì±„",
                # ê¸°ê´€
                "ì™¸êµë¶€", "í†µì¼ë¶€", "êµ­ë°©ë¶€", "ì²­ì™€ëŒ€", "êµ­íšŒ",
                "UN", "ìœ ì—”", "UNDP", "UNESCO", "WHO", "UNICEF",
                "NATO", "EU", "ASEAN", "APEC", "G20", "OECD",
                "ì„¸ê³„ì€í–‰", "IMF", "WTO",
                # NGO
                "NGO", "INGO", "ì˜¥ìŠ¤íŒœ", "ì›”ë“œë¹„ì „", "êµ¿ë„¤ì´ë²„ìŠ¤",
                "êµ­ê²½ì—†ëŠ”ì˜ì‚¬íšŒ", "KOICA", "ì›”ë“œí”„ë Œì¦ˆ",
                # ì§ë¬´
                "ê³µê³µì™¸êµ", "ë‹¤ìì™¸êµ", "ê²½ì œì™¸êµ", "ë¬¸í™”ì™¸êµ",
                "ê°œë°œí˜‘ë ¥", "ODA", "í‰í™”", "ê°ˆë“±í•´ê²°", "ì¸ê¶Œ",
                "ì •ì±…ì—°êµ¬", "ì‹±í¬íƒ±í¬",
                # ì§€ì—­
                "ë¶í•µ", "í•œë°˜ë„", "ë™ë¶ì•„", "ì¤‘ë™", "í•œë¯¸ë™ë§¹",
                # ê¸°íƒ€
                "ëŒ€ì‚¬ê´€", "ì˜ì‚¬ê´€", "êµ­ì œíšŒì˜", "SDGs", "ê¸°í›„ë³€í™”"
            ]
        }
    
    def load_existing_data(self) -> Dict:
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        try:
            with open('data.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {"lastUpdated": "", "jobs": [], "contests": []}
    
    def categorize(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        text_lower = text.lower()
        scores = {category: 0 for category in self.keywords}
        
        for category, keywords in self.keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    scores[category] += 1
        
        max_score = max(scores.values())
        if max_score > 0:
            return max(scores, key=scores.get)
        return None  # ê´€ë ¨ ì—†ìŒ
    
    def is_duplicate(self, new_item: Dict, existing_items: List[Dict]) -> bool:
        """ì¤‘ë³µ í™•ì¸"""
        for item in existing_items:
            if item.get('url') == new_item.get('url'):
                return True
            if item.get('title') == new_item.get('title'):
                return True
        return False
    
    def parse_rss_feeds(self):
        """RSS í”¼ë“œ ìˆ˜ì§‘"""
        print("ğŸ“¡ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
        
        # ì‹¤ì œ RSS URLë¡œ êµì²´í•˜ì„¸ìš”
        rss_sources = {
            "jobs": [
                "https://www.saramin.co.kr/zf_user/help/live/rss",
            ],
            "contests": [
                "https://www.wevity.com/index_rss.php",
            ]
        }
        
        for feed_url in rss_sources["jobs"]:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries[:20]:
                    title = entry.get('title', '')
                    description = entry.get('summary', '')
                    
                    category = self.categorize(title + " " + description)
                    if not category:  # ê´€ë ¨ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                        continue
                    
                    job = {
                        "id": len(self.data["jobs"]) + self.new_items_count + 1,
                        "title": title,
                        "company": entry.get('author', 'ë¯¸ì •'),
                        "category": category,
                        "type": "ì±„ìš©",
                        "deadline": self.extract_deadline(entry),
                        "url": entry.get('link', ''),
                        "tags": self.extract_tags(title),
                        "new": True
                    }
                    
                    if not self.is_duplicate(job, self.data["jobs"]):
                        self.data["jobs"].append(job)
                        self.new_items_count += 1
                        print(f"âœ… ìƒˆ ì±„ìš©: {title[:40]}...")
            except Exception as e:
                print(f"âŒ RSS ì˜¤ë¥˜: {e}")
        
        for feed_url in rss_sources["contests"]:
            try:
                feed = feedparser.parse(feed_url)
                for entry in feed.entries[:20]:
                    title = entry.get('title', '')
                    description = entry.get('summary', '')
                    
                    category = self.categorize(title + " " + description)
                    if not category:
                        continue
                    
                    contest = {
                        "id": len(self.data["contests"]) + self.new_items_count + 1,
                        "title": title,
                        "organizer": entry.get('author', 'ë¯¸ì •'),
                        "category": category,
                        "type": "ê³µëª¨ì „",
                        "deadline": self.extract_deadline(entry),
                        "prize": self.extract_prize(description),
                        "url": entry.get('link', ''),
                        "tags": self.extract_tags(title),
                        "new": True
                    }
                    
                    if not self.is_duplicate(contest, self.data["contests"]):
                        self.data["contests"].append(contest)
                        self.new_items_count += 1
                        print(f"âœ… ìƒˆ ê³µëª¨ì „: {title[:40]}...")
            except Exception as e:
                print(f"âŒ RSS ì˜¤ë¥˜: {e}")
    
    def extract_deadline(self, entry) -> str:
        """ë§ˆê°ì¼ ì¶”ì¶œ"""
        future_date = datetime.now() + timedelta(days=30)
        return future_date.strftime('%Y-%m-%d')
    
    def extract_prize(self, text: str) -> str:
        """ìƒê¸ˆ ì¶”ì¶œ"""
        patterns = [r'(ëŒ€ìƒ|ìƒê¸ˆ)[:\s]*([0-9,]+ë§Œ?\s?ì›)', r'([0-9,]+ë§Œ?\s?ì›)']
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)
        return ""
    
    def extract_tags(self, text: str) -> List[str]:
        """íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        tag_keywords = {
            "ì‹ ì…", "ê²½ë ¥", "ì¸í„´", "ëŒ€í•™ìƒ", "ì²­ë…„",
            "ì„œìš¸", "ë¶€ì‚°", "ì˜¨ë¼ì¸", "ì•„ì´ë””ì–´"
        }
        text_lower = text.lower()
        for keyword in tag_keywords:
            if keyword in text_lower:
                tags.append(keyword)
        return tags[:5]
    
    def clean_old_data(self):
        """ì§€ë‚œ ê³µê³  ì œê±°"""
        print("ğŸ§¹ ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬ ì¤‘...")
        today = datetime.now()
        
        self.data["jobs"] = [
            job for job in self.data["jobs"]
            if datetime.strptime(job['deadline'], '%Y-%m-%d') >= today
        ]
        
        self.data["contests"] = [
            contest for contest in self.data["contests"]
            if datetime.strptime(contest['deadline'], '%Y-%m-%d') >= today
        ]
    
    def save_data(self):
        """ë°ì´í„° ì €ì¥"""
        self.data['lastUpdated'] = datetime.now().isoformat()
        
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ!")
        print(f"ğŸ“Š ì±„ìš©: {len(self.data['jobs'])}ê±´")
        print(f"ğŸ† ê³µëª¨ì „: {len(self.data['contests'])}ê±´")
        print(f"ğŸ†• ì‹ ê·œ: {self.new_items_count}ê±´")

def main():
    print("=" * 50)
    print("ğŸ¤– ìë™ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 50)
    
    scraper = CareerScraper()
    scraper.parse_rss_feeds()
    scraper.clean_old_data()
    scraper.save_data()
    
    print("\nâœ… ìˆ˜ì§‘ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
